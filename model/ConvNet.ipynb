{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mappings = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "    3: 'D',\n",
    "    4: 'E',\n",
    "    5: 'F',\n",
    "    6: 'G',\n",
    "    7: 'H',\n",
    "    8: 'I',\n",
    "    9: 'J',\n",
    "    10: 'K',\n",
    "    11: 'L',\n",
    "    12: 'M',\n",
    "    13: 'N',\n",
    "    14: 'O',\n",
    "    15: 'P',\n",
    "    16: 'Q',\n",
    "    17: 'R',\n",
    "    18: 'S',\n",
    "    19: 'T',\n",
    "    20: 'U',\n",
    "    21: 'V',\n",
    "    22: 'W',\n",
    "    23: 'X',\n",
    "    24: 'Y',\n",
    "    25: 'Z',\n",
    "    26: 'del',\n",
    "    27: 'space',\n",
    "    28: 'nothing'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size = 5):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, self.kernel_size ) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, self.kernel_size ) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, self.kernel_size )\n",
    "  \n",
    "\n",
    "        x = torch.randn(IMAGE_SIZE,IMAGE_SIZE).view(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 256) \n",
    "        self.fc3 = nn.Linear(256,29)\n",
    "\n",
    "    def convs(self, x):\n",
    "        # average pooling over 2x2\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        \n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x)) # bc this is our output layer. No activation here.\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net(kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=18432, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=29, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('all_plus_tarun_v2.npy',allow_pickle=True)            \n",
    "np.random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()   \n",
    "import gc\n",
    "try: \n",
    "    del X\n",
    "    del y\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(training_data[0][0]).view(-1,1,IMAGE_SIZE,IMAGE_SIZE).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35249\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "X= X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "VAL_PCT = 0.33\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71567 35249\n"
     ]
    }
   ],
   "source": [
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "print(len(train_X),len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35249, 29])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_test, y_test):\n",
    "    outputs = net(x_test)\n",
    "   \n",
    "    acc = acc_score(outputs,y_test)\n",
    "    loss = loss_function(outputs,y_test)\n",
    "    return acc,loss\n",
    "def acc_score(outputs, y_test):\n",
    "    matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y_test)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 ====> Train_loss:0.0541 ====>  Train_acc:0.6\n",
      "epoch:0 ====> val_loss:0.0503 ====>  val_acc:0.6719\n",
      "epoch:1 ====> Train_loss:0.031 ====>  Train_acc:0.8\n",
      "epoch:1 ====> val_loss:0.0335 ====>  val_acc:0.8047\n",
      "epoch:2 ====> Train_loss:0.0224 ====>  Train_acc:0.8667\n",
      "epoch:2 ====> val_loss:0.0241 ====>  val_acc:0.8438\n",
      "epoch:3 ====> Train_loss:0.0257 ====>  Train_acc:0.8667\n",
      "epoch:3 ====> val_loss:0.0266 ====>  val_acc:0.8672\n",
      "epoch:4 ====> Train_loss:0.0148 ====>  Train_acc:0.9333\n",
      "epoch:4 ====> val_loss:0.0263 ====>  val_acc:0.8594\n",
      "epoch:5 ====> Train_loss:0.0333 ====>  Train_acc:0.8\n",
      "epoch:5 ====> val_loss:0.0207 ====>  val_acc:0.8828\n",
      "epoch:6 ====> Train_loss:0.0089 ====>  Train_acc:0.9333\n",
      "epoch:6 ====> val_loss:0.0126 ====>  val_acc:0.9141\n",
      "epoch:7 ====> Train_loss:0.0149 ====>  Train_acc:0.8667\n",
      "epoch:7 ====> val_loss:0.0235 ====>  val_acc:0.8438\n",
      "epoch:8 ====> Train_loss:0.0077 ====>  Train_acc:0.9333\n",
      "epoch:8 ====> val_loss:0.0126 ====>  val_acc:0.9219\n",
      "epoch:9 ====> Train_loss:0.0046 ====>  Train_acc:1.0\n",
      "epoch:9 ====> val_loss:0.0191 ====>  val_acc:0.8672\n",
      "epoch:10 ====> Train_loss:0.0058 ====>  Train_acc:1.0\n",
      "epoch:10 ====> val_loss:0.0153 ====>  val_acc:0.9219\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_function = nn.BCELoss()\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 11\n",
    "VAL_BATCH = 128\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(0,len(train_X),BATCH_SIZE):\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs ,batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = acc_score(outputs,batch_y)\n",
    "    print(f'epoch:{epoch} ====> Train_loss:{round(loss.item(),4)} ====>  Train_acc:{round(acc,4)}')\n",
    "    if epoch % 1 == 0:\n",
    "        rand = random.randint(0,len(test_X)-VAL_BATCH)\n",
    "        val_acc,val_loss = test(test_X[rand:rand+VAL_BATCH],test_y[rand:rand+VAL_BATCH])\n",
    "        print(f'epoch:{epoch} ====> val_loss:{round(val_loss.item(),4) } ====>  val_acc:{round(val_acc,4)}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy P\n",
    "correct = 0 \n",
    "total = 0 \n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1,1,IMAGE_SIZE,IMAGE_SIZE))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        if predicted_class == real_class:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "    print('Accuracy:',round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy (class-wise)\n",
    "from collections import Counter\n",
    "outputs = []\n",
    "correct_outputs = []\n",
    "with torch.no_grad():\n",
    "    for test_case in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[test_case])\n",
    "        real_label = label_mappings[real_class.item()]\n",
    "        \n",
    "        model_output = net(test_X[test_case].view(-1,1, IMAGE_SIZE,IMAGE_SIZE))\n",
    "        predicted_label = label_mappings[torch.argmax(model_output).item()]\n",
    "        outputs.append(real_label)\n",
    "        if predicted_label == real_label:\n",
    "            correct_outputs.append(predicted_label)\n",
    "            \n",
    "class_wise_accuracies = Counter(correct_outputs)\n",
    "class_counts = Counter(outputs)\n",
    "average_accuracy = 0\n",
    "for key, corr_otpt_cnt in class_wise_accuracies.items():\n",
    "    class_accuracy = corr_otpt_cnt\n",
    "    class_count = class_counts[key]\n",
    "    average_acc = class_accuracy / class_count\n",
    "    average_accuracy+=average_acc\n",
    "print(f\"Class wise accuracy : {round((average_accuracy / len(label_mappings)),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_wise_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'fully_connected_all_data_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random \n",
    "index_to_test = random.randint(0,24)\n",
    "label_to_show = test_y[index_to_test]\n",
    "img_to_show = test_X[index_to_test][0]\n",
    "img_to_show_cpu = torch.Tensor.cpu(img_to_show)\n",
    "\n",
    "inference = torch.argmax(net(img_to_show.view(-1,1,IMAGE_SIZE,IMAGE_SIZE)))\n",
    "\n",
    "\n",
    "print(f'label_to_show: {label_mappings[torch.argmax(label_to_show).item()]}')\n",
    "print(f'inference: {inference}')\n",
    "print(f'inference_label: {label_mappings[inference.item()]}')\n",
    "cv2.imshow(\"re\",np.array(img_to_show_cpu))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL_TO_SPEECH",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
