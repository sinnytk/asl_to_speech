{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mappings = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "    3: 'D',\n",
    "    4: 'E',\n",
    "    5: 'F',\n",
    "    6: 'G',\n",
    "    7: 'H',\n",
    "    8: 'I',\n",
    "#     9: 'J',\n",
    "    10: 'K',\n",
    "    11: 'L',\n",
    "    12: 'M',\n",
    "    13: 'N',\n",
    "    14: 'O',\n",
    "    15: 'P',\n",
    "    16: 'Q',\n",
    "    17: 'R',\n",
    "    18: 'S',\n",
    "    19: 'T',\n",
    "    20: 'U',\n",
    "    21: 'V',\n",
    "    22: 'W',\n",
    "    23: 'X',\n",
    "    24: 'Y',\n",
    "#     25: 'Z',\n",
    "#     26: 'del',\n",
    "#     27: 'space',\n",
    "#     28: 'nothing'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, kernel_size = 5):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, self.kernel_size ) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, self.kernel_size ) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, self.kernel_size )\n",
    "        self.conv4 = nn.Conv2d(128, 256, self.kernel_size )\n",
    "        self.conv5 = nn.Conv2d(256, 512, self.kernel_size )\n",
    "        self.norm2 = nn.BatchNorm2d(512)\n",
    "  \n",
    "\n",
    "        x = torch.randn(IMAGE_SIZE,IMAGE_SIZE).view(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 256) \n",
    "        self.norm1d = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256,len(label_mappings))\n",
    "\n",
    "    def convs(self, x):\n",
    "        # average pooling over 2x2\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = self.norm1(x)\n",
    "        x = F.avg_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv4(x)), (2, 2))\n",
    "        x = F.avg_pool2d(F.relu(self.conv5(x)), (2, 2))\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x)) # bc this is our output layer. No activation here.\n",
    "        x = self.norm1d(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "net = Net(kernel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv5): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (fc1): Linear(in_features=4608, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (norm1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('mediapipe_all_data.npy',allow_pickle=True)            \n",
    "np.random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()   \n",
    "import gc\n",
    "try: \n",
    "    del X\n",
    "    del y\n",
    "except NameError:\n",
    "    pass\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100, 100])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(training_data[0][0]).view(-1,1,IMAGE_SIZE,IMAGE_SIZE).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "X= X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "torch.save(X, 'piped_X_tensor.pt')\n",
    "torch.save(y, 'piped_y_tensor.pt')\n",
    "# X = torch.load('X_tensor.pt')\n",
    "# y = torch.load('y_tensor.pt')\n",
    "# VAL_PCT = 0.33\n",
    "# val_size = int(len(X)*VAL_PCT)\n",
    "# print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 8.00 GiB total capacity; 5.13 GiB already allocated; 1019.62 MiB free; 5.15 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-60b1a0cba3ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 8.00 GiB total capacity; 5.13 GiB already allocated; 1019.62 MiB free; 5.15 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "print(len(train_X),len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "71567/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0.0\n",
    "for x in range(560):\n",
    "    sum+=0.9\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x_test, y_test):\n",
    "    outputs = net(x_test)\n",
    "   \n",
    "    acc = acc_score(outputs,y_test)\n",
    "    loss = loss_function(outputs,y_test)\n",
    "    return acc,loss\n",
    "def acc_score(outputs, y_test):\n",
    "    matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y_test)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 ====> Train_loss:0.0294 ====>  Train_acc:0.8189\n",
      "epoch:0 ====> val_loss:0.0352 ====>  val_acc:0.7734\n",
      "epoch:1 ====> Train_loss:0.0185 ====>  Train_acc:0.9291\n",
      "epoch:1 ====> val_loss:0.0189 ====>  val_acc:0.8984\n",
      "epoch:2 ====> Train_loss:0.0114 ====>  Train_acc:0.9528\n",
      "epoch:2 ====> val_loss:0.0156 ====>  val_acc:0.9375\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_function = nn.BCELoss()\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "VAL_BATCH = 128\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(0,len(train_X),BATCH_SIZE):\n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        \n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs ,batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = acc_score(outputs,batch_y)\n",
    "    print(f'epoch:{epoch} ====> Train_loss:{round(loss.item(),4)} ====>  Train_acc:{round(acc,4)}')\n",
    "    if epoch % 1 == 0:\n",
    "        rand = random.randint(0,len(test_X)-VAL_BATCH)\n",
    "        val_acc,val_loss = test(test_X[rand:rand+VAL_BATCH],test_y[rand:rand+VAL_BATCH])\n",
    "        print(f'epoch:{epoch} ====> val_loss:{round(val_loss.item(),4) } ====>  val_acc:{round(val_acc,4)}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38771/38771 [00:29<00:00, 1311.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Accuracy P\n",
    "correct = 0 \n",
    "total = 0 \n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1,1,IMAGE_SIZE,IMAGE_SIZE))[0]\n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        if predicted_class == real_class:\n",
    "            correct+=1\n",
    "        total+=1\n",
    "    print('Accuracy:',round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_X[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(test_X[0].view(-1,1,IMAGE_SIZE,IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(test_X[0:2].view(-1,1,IMAGE_SIZE,IMAGE_SIZE))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy (class-wise)\n",
    "from collections import Counter\n",
    "outputs = []\n",
    "correct_outputs = []\n",
    "with torch.no_grad():\n",
    "    for test_case in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[test_case])\n",
    "        real_label = label_mappings[real_class.item()]\n",
    "        \n",
    "        model_output = net(test_X[test_case].view(-1,1, IMAGE_SIZE,IMAGE_SIZE))\n",
    "        predicted_label = label_mappings[torch.argmax(model_output).item()]\n",
    "        outputs.append(real_label)\n",
    "        if predicted_label == real_label:\n",
    "            correct_outputs.append(predicted_label)\n",
    "            \n",
    "class_wise_accuracies = Counter(correct_outputs)\n",
    "class_counts = Counter(outputs)\n",
    "average_accuracy = 0\n",
    "for key, corr_otpt_cnt in class_wise_accuracies.items():\n",
    "    class_accuracy = corr_otpt_cnt\n",
    "    class_count = class_counts[key]\n",
    "    average_acc = class_accuracy / class_count\n",
    "    average_accuracy+=average_acc\n",
    "print(f\"Class wise accuracy : {round((average_accuracy / len(label_mappings)),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds = [net(i.view(-1,1,IMAGE_SIZE,IMAGE_SIZE).to(device)).cpu().detach().numpy() for i in test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "f1  = f1_score(np.argmax(preds),np.argmax(y_test))\n",
    "# acc  = accuracy_score(preds[:100],y_test[:100])\n",
    "    \n",
    "print(f'F1 Score for this model : {f1}')\n",
    "# print(f'accuracy Score for this model : {acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_wise_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'fully_connected_all_data_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random \n",
    "index_to_test = random.randint(0,24)\n",
    "label_to_show = test_y[index_to_test]\n",
    "img_to_show = test_X[index_to_test][0]\n",
    "img_to_show_cpu = torch.Tensor.cpu(img_to_show)\n",
    "\n",
    "inference = torch.argmax(net(img_to_show.view(-1,1,IMAGE_SIZE,IMAGE_SIZE)))\n",
    "\n",
    "\n",
    "print(f'label_to_show: {label_mappings[torch.argmax(label_to_show).item()]}')\n",
    "print(f'inference: {inference}')\n",
    "print(f'inference_label: {label_mappings[inference.item()]}')\n",
    "cv2.imshow(\"re\",np.array(img_to_show_cpu))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL_TO_SPEECH",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
